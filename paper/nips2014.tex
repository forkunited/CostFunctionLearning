\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\newcommand{\bmcomment}[1]{\textcolor{green}{\textsc{\textbf{[#1 --wvm]}}}}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Learning when to try hard at learning}

\author{
Bill McDowell\thanks{Alternative email: \texttt{forkunited@gmail.com}}
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{wmcdowel@cs.cmu.edu} \\
\And
Noah A.~Smith
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{nasmith@cs.cmu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\bmcomment{Probably need a better title.}

\bmcomment{Noah, I put your name second because your name seems 
to be last on all of your recent papers.  I don't care which our names is
first though, so feel free to swap if you want.}

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\section{Introduction}

\section{Background}

%- SVM have a cost function
%- Want to learn this
%- Explain intuition
%- Desired properties of the cost function

\section{Cost Learning Model}

%- With/without n
%- Intuition
%- Properties
%- Values of n

\section{Experiments}
%- Model implementation
%- Pegasos
%- Data 
%- 20newsgroups
%- Reuters
%- Performance
%- Learned cost weights, relation to hierarchies

\section{Discussion}

\subsection{Related Literature}

% See possible reference list below

\subsection{Future Work}

% Other choices for n
% Other measures of 'difficulty'
% Structured version
% Relationship between model 'difficulty' and 'task' difficulty
% Characterize properties of data/features that determine whether cost learning is useful

%\subsubsection*{Acknowledgments}

%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments go at the end of the paper. Do not include 
%acknowledgments in the anonymized submission, only in the 
%final paper. 

\subsubsection*{References}
% Here are some references to possibly include
%
% TODO: Things about convexity
%
% Structured Perceptron
% Collins, Michael. ``Discriminative training methods for 
% hidden markov models: Theory and experiments with perceptron algorithms.'' In Proceedings 
% of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pp. 1-8. Association for Computational Linguistics, 2002.
% 
% Structured SVM
% Tsochantaridis, Ioannis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun, and Yoram Singer. 
% ``Large margin methods for structured and interdependent output variables.'' Journal of Machine Learning Research 6, no. 9 (2005).
%
% Multiclass SVM
% Crammer, Koby, and Yoram Singer. 
% ``On the algorithmic implementation of multiclass kernel-based vector machines.'' The Journal of Machine Learning Research 2 (2002): 265-292.
%
% TODO:20newsgroups 
% 
% TODO:Reuters
%
% SGD algorithm implementation/learning rate
% Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew Cotter. 
% ``Pegasos: Primal estimated sub-gradient solver for svm.'' Mathematical programming 127, no. 1 (2011): 3-30.
%
% Inspiration for objective function (select cost weights to turn on)
% Kumar, M. Pawan, Benjamin Packer, and Daphne Koller. ``Self-Paced Learning for Latent Variable Models.'' In NIPS, vol. 1, p. 3. 2010.
%
% Curriculum Learning
% Bengio, Yoshua, JÃ©rÃŽme Louradour, Ronan Collobert, and Jason Weston
%. ``Curriculum learning.'' In Proceedings of the 26th annual international conference on machine learning, pp. 41-48. ACM, 2009.
%
% Confident weighted learning
% Dredze, Mark, Koby Crammer, and Fernando Pereira. ``Confidence-weighted linear classification.'' In Proceedings 
% of the 25th international conference on Machine learning, pp. 264-271. ACM, 2008.
%
% Some other things from http://www.cs.cmu.edu/~nasmith/papers/career-proposal-2010.pdf 
%   - Hidden variable learning by state splitting?
%   - Finite state output encodings
%
% Here's how things are formatted in the NIPS template:
%
%
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.

%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}

\end{document}
