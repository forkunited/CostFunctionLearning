\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage[sort&compress]{natbib}
\usepackage{natbibspacing}

\usetikzlibrary{arrows}

\bibliographystyle{unsrt}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\bmcomment}[1]{\textcolor{blue}{\textsc{\textbf{[#1 --bm]}}}}
\newcommand{\nascomment}[1]{\textcolor{red}{\textsc{\textbf{[#1 --nas]}}}}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Learning when to try hard at learning}

\author{
Bill McDowell\thanks{Alternative email: \texttt{forkunited@gmail.com}}
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{wmcdowel@cs.cmu.edu} \\
\And
Noah A.~Smith
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{nasmith@cs.cmu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\bmcomment{Probably need a better title.}

\bmcomment{Noah, I put your name second because your name seems 
to be last on all of your recent papers.  I don't care which our names is
first though, so feel free to swap if you want.}

\begin{abstract}
\bmcomment{I have trouble writing this part.}
\end{abstract}

\section{Introduction}

\bmcomment{General ideas about cost learning abstracted away
from the SVM details in the next section...?}

\section{Background}

\bmcomment{I introduced SVMs through the quadratic programming
formulation because it seemed easier to summarize margin maximization
across all the relevant SVM variants that way while staying true to the literature... 
but maybe I should have just kept it in the form
of an unconstrained optimization problem for continuity with the rest
of this paper?}

\bmcomment{Cite some paper on using binary svms for multiclass}

\bmcomment{Cite multiclass svm}

\bmcomment{Cite something for margin maximization}

\bmcomment{Cite structured svm}

\bmcomment{Cite example of unreliable output labels}

\bmcomment{Also cite taskar et al for 'margin rescaling'?}

\bmcomment{Might want to give some motivating examples? Not sure if that's
necessary or not. One example of multiclass domain, and one example where
cost functions are used in structured domains}

\bmcomment{Mention examples of measures of 'difficulty'? Cite.}

For the multiclass classification problem, we are given a set of
$m$ labelled training data instances
$D=\{(\mathbf{x}_1,\mathbf{y}_1),\hdots,(\mathbf{x}_m, \mathbf{y}_m)\}\subset\mathcal{X}\times\mathcal{Y}$ 
where each example $\mathbf{x}_i$ is assigned label 
$\mathbf{y}_i$, and we want to learn a function 
$f:\mathcal{X}\rightarrow\mathcal{Y}$ which gives the correct
label for any input taken from $\mathcal{X}$.  Past work has developed
a multiclass support vector machine (SVM) which generalizes the 
concept of margin maximization employed by classical SVMs for binary
classification tasks. In particular, assuming that
$\mathbf{g}:\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}^k$ 
computes a featurized representation of an input-output pair, the 
multiclass SVM gives a score to a labelled instance $(\mathbf{x},\mathbf{y})$ 
as:

\begin{equation}
\label{score}
F(\mathbf{x},\mathbf{y};\mathbf{w})= \mathbf{w}^\top \mathbf{g}(\mathbf{x},\mathbf{y})
\end{equation}

And makes predictions according to:

\begin{equation}
\label{prediction}
\hat{f}(\mathbf{x};\mathbf{w})=\argmax_{\mathbf{y}'\in\mathcal{Y}} F(\mathbf{x},\mathbf{y}';\mathbf{w})
\end{equation}

Where the feature weights $\mathbf{w}$ are given by the
solution to the following soft-margin maximizing quadratic program:

\begin{equation}
\begin{split}
& \min_{\mathbf{\xi}\geq 0, \mathbf{w}}\big(\frac{\lambda_2}{2}\|\mathbf{w}\|_2^2+\frac{1}{m}\sum_{i=1}^m\xi_i\big) \\
& \text{s.t.     } \forall i : \forall \mathbf{y}\in\mathcal{Y}\backslash\mathbf{y}_i : F(\mathbf{x}_i,\mathbf{y}_i;\mathbf{w})-F(\mathbf{x}_i,\mathbf{y};\mathbf{w})\geq 1-\xi_i
\end{split}
\end{equation}

This quadratic program chooses weights that minimize
the number of misclassified instances while simultaneously
trying to increase the margin between the scores of 
correct and incorrect labels, and the $\lambda_2$
hyper-parameter determines the relative importance of
these two goals.\footnote{This SVM and all other models in
this paper can include non-regularized bias terms which
we omit from our descriptions for readability, but these
biases were included in the implementations we used for
our experiments.}

Tsochantaridis et al. further generalized the margin maximization
to make sense for domains where some prediction mistakes are more 
costly than others (especially domains where $\mathcal{Y}$ contains
a large number of structured outputs) \citep{tsochantaridis2004support}.  They introduce a function 
$\Delta:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}$ that
determines the cost
$\Delta(\mathbf{y},\mathbf{\hat{y}})$ of predicting label 
$\mathbf{\hat{y}}$ when then correct label is $\mathbf{y}$.  
The $\Delta(\mathbf{y},\mathbf{\hat{y}})$ is larger for more 
inaccurate $\mathbf{\hat{y}}$ predictions, and 
$\Delta(\mathbf{y},\mathbf{y})=0$, giving no cost for a correct
prediction. Tsiochantaridis et al. incorporate 
the cost function into the SVM learning by modifying the 
multiclass SVM quadratic
program through 'margin re-scaling' as:\footnote{Tsiochantaridis et al. 
also introduce an alternative 'slack re-scaling' technique, but the 
present paper mainly builds off of the 'margin re-scaling' approach.}

\begin{equation}
\label{marginRescaling}
\begin{split}
& \min_{\mathbf{\xi}\geq 0, \mathbf{w}}\big(\frac{\lambda_2}{2}\|\mathbf{w}\|_2^2+\frac{1}{m}\sum_{i=1}^m\xi_i\big) \\
& \text{s.t.     } \forall i : \forall \mathbf{y}\in\mathcal{Y}\backslash\mathbf{y}_i : F(\mathbf{x}_i,\mathbf{y}_i;\mathbf{w})-F(\mathbf{x}_i,\mathbf{y};\mathbf{w})\geq \Delta(\mathbf{y}_i,\mathbf{y})-\xi_i
\end{split}
\end{equation}

Intuitively, this margin re-scaling quadratic program allows 
the model to choose margins between between scores of 
$\mathbf{y}_i$ and $\mathbf{\hat{y}}_i$  proportional to
the prediction cost $\Delta(\mathbf{y}_i,\mathbf{\hat{y}})$.
This choice biases the model toward making low cost 
predictions over high cost predictions.

Whereas previous work assumes that we hand-select 
domain-specific cost functions that seem like 
good measures of the distances between labels, we
propose learning this function from the data jointly with
the prediction function $f$.  Similarly to the hand-selected
cost functions, the learned cost functions
should reflect some notion of the distance between two
labels, but as seen from the perspective of the model and its
features rather than from the perspective of the person 
engineering the model. In other words, the value of 
$\Delta(\mathbf{y}, \mathbf{\hat{y}})$ should
be proportional to the ease with which the model  
discriminates between $\mathbf{y}$ and $\mathbf{\hat{y}}$
given its features.  This choice of cost gives the model the
freedom to shift its weights so that there are wider margins
between labels which it can easily discriminate, and smaller
margins between labels which it has difficulty discriminating.
Such a policy is particularly useful when there are classes
of incorrect predictions that are difficult or impossible for 
the model to systematically resolve due to unreliably 
annotated output labels or a 
choice of features that is insufficient for the task.

There are several ways that we might quantify 
the ease with which the model discriminates between two labels,
but for now, we will keep the notion of 'easiness' fuzzy while
establishing its relation to the cost function.
We propose that an incorrect prediction 
$\mathbf{\hat{y}}$ where the actual label is $\mathbf{y}$ can 
fall into some number of 'incorrect prediction classes', and
the overall cost of the incorrect prediction is the sum over the
easiness of resolving (shrinking) each of these classes.  More
formally, let $\mathcal{S}\subseteq 2^{\mathcal{Y}^2}$ be a 
collection of incorrect prediction classes that collectively 
exhausts $\mathcal{Y}^2$.  Assume that the 'easiness' with 
which a model of type $\mathcal{M}$ (e.g. SVM) given features 
$\mathbf{g}$ and data $D$ resolves incorrect prediction 
class $S\in\mathcal{S}$ is given by 
$\mathcal{E}(S,\mathcal{M},\mathbf{g},D)\in\mathbb{R}$.
Then, we assume that the cost is given by:

\begin{equation}
\Delta(\mathbf{y},\mathbf{\hat{y}})=\sum_{S\in\mathcal{S}}\mathcal{E}(S,\mathcal{M},\mathbf{g}, D)\mathbbm{1}((\mathbf{y},\mathbf{\hat{y}})\in S)=
\mathbf{\mathcal{E}}^\top \mathbf{\mathcal{S}}(\mathbf{y},\mathbf{\hat{y}})
\end{equation}

There are many possible choices for prediction classes $\mathcal{S}$, but
in this paper, we focus on the following two:

\begin{equation}
\begin{split}
& \mathcal{S}_{[\mathcal{Y}]^2} = \{ S_{\{\mathbf{y},\mathbf{y}'\}} | \mathbf{y},\mathbf{y}'\in\mathcal{Y}\} \\
\text{ where } & S_{\{\mathbf{y},\mathbf{y}'\}}=
\{ (\mathbf{y}_1, \mathbf{y}_2) | (\mathbf{y}_1=\mathbf{y}\wedge \mathbf{y}_2=\mathbf{y}')\vee(\mathbf{y}_1=\mathbf{y}'\wedge \mathbf{y}_2=\mathbf{y}) \} \\
\end{split}
\end{equation}

\begin{equation}
\mathcal{S}_{\mathcal{Y}^2} = \{ S_{(\mathbf{y},\mathbf{y}')} | \mathbf{y},\mathbf{y}'\in\mathcal{Y}\}\text{ where }S_{(\mathbf{y},\mathbf{y}')}=\{ (\mathbf{y},\mathbf{y}') \} \\
\end{equation}

$\mathcal{S}_{[\mathcal{Y}]^2}$ contains a prediction class for every
unordered pair of labels, and $\mathcal{S}_{\mathcal{Y}^2}$ contains
a prediction class for every ordered pair.  We might choose 
$\mathcal{S}_{[\mathcal{Y}]^2}$ when factoring the cost function if
we believe it is equally easy to resolve misclassifications of 
$\mathbf{y}$ as $\mathbf{y}'$ and misclassifications of $\mathbf{y}'$
as $\mathbf{y}$.  Otherwise, if we suspect these two incorrect 
prediction types to differ in easiness, then we might choose 
$\mathcal{S}_{\mathcal{Y}^2}$.

\section{A cost learning model}
\label{costLearningModel}

We desire a model which approximates 
$\mathcal{E}(S,\mathcal{M},\mathbf{g}, D)$ (for some notion of 
'easiness') to estimate prediction costs while simultaneous learning 
feature weights $\mathbf{w}$.   Our proposal follows the intuition 
that the easiness 
$\mathcal{E}(S,\mathcal{M},\mathbf{g},D)$ of prediction class $S$ for 
model class $\mathcal{M}$ is related to the size of the set:

\begin{equation}
S_{\mathcal{M},\mathbf{g},D}=\{(\textbf{x}_i,\textbf{y}_i) | (\mathbf{y}_i,\hat{f}^\Delta_{\mathcal{M},\mathbf{g},D}(\mathbf{x}_i;\mathbf{w}_{\mathcal{M},\mathbf{g},D}))\in S\text{ and }(\mathbf{x}_i,\mathbf{y}_i)\in D\}
\end{equation}

Where $\mathbf{w}_{\mathcal{M},\mathbf{g},D}$ are the learned feature weights,
and $\hat{f}^\Delta_{\mathcal{M},\mathbf{g},D}$ is the model's prediction
function augmented with the cost:

\begin{equation}
\hat{f}^\Delta_{\mathcal{M},\mathbf{g},D}(\mathbf{x}_i;\mathbf{w})=\argmax_{\mathbf{y}\in\mathcal{Y}}\Big( F(\mathbf{x}_i,\mathbf{y};\mathbf{w})
+\Delta(\mathbf{y}_i,\mathbf{y})\Big)
\end{equation}

The size of $S_{\mathcal{M},\mathbf{g},D}$ is the number of
training examples with margin violations in class $S$. If
the size of this set is large, we might infer that the
model has trouble shrinking it, and so it's not 'easy'.  This 
might lead us to conclude that $S_{\mathcal{M},\mathbf{g},D}$ 
tends to shrink with 'easiness'.  However,
for many data sets and choices of $\mathcal{S}$, the size 
of each $S_{\mathcal{M},\mathbf{g},D}$ can be inherently
biased by the data independently of 'easiness'.  For example, 
for $S_{\{\mathbf{y},\mathbf{y}'\}}\in\mathcal{S}_{[\mathcal{Y}]^2}$, 
the size of $S_{\{\mathbf{y},\mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ 
is biased by the number of examples in the training data which have 
output labels $\mathbf{y}$ and $\mathbf{y}'$--if there are few 
training examples of labels 
$\mathbf{y},\mathbf{y}'\in\mathcal{Y}$, then the size of 
$S_{\{\mathbf{y}, \mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ 
will necessarily be small relative to other prediction classes.
Furthermore, we expect 
output labels which occur infrequently in the training data to 
be more difficult for the model to predict correctly, so this
will lead to the size of 
$S_{\{\mathbf{y},\mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ 
increasing with the 'easiness' of 
$S_{\{\mathbf{y},\mathbf{y}'\}}$ which is opposite the 
conclusion that that we draw if we think of the size of 
$S_{\{\mathbf{y},\mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ as
increasing due to the model's difficulty in shrinking it.  In general, 
this suggests that if we want the size of $S_{\mathcal{M},\mathbf{g},D}$
to vary with easiness, we need to normalize it to account for 
properties of the training data that introduce irrelevant
biases.  

\subsection{A measure of 'easiness'}

\bmcomment{Might want to discuss other possibilities, or generally
refer to other possibilities in future work}

The above observations suggest the following as a possible 
measure of 'easiness':

\begin{equation}
\label{easiness}
\mathcal{E}(S,\mathcal{M},\mathbf{g},D)=\max\bigg(0, 1-\frac{|S_{\mathcal{M},\mathbf{g},D}|}{n_{S,\mathcal{M},\mathbf{g},D}}\bigg)
\end{equation}

Where $n_{S,\mathcal{M},\mathbf{g},D}$ is a normalization constant which
gives the maximum possible value we expect for the size of 
$S_{\mathcal{M},\mathbf{g},D}$, accounting for irrelevant biases
introduced by the data as discussed above.  This measure of easiness 
is in $[0,1]$, and it has the property that if 
$|S_{\mathcal{M},\mathbf{g},D}|\geq n_{S,\mathcal{M},\mathbf{g},D}$,
then $\mathcal{E}(S,\mathcal{M},\mathbf{g},D)=0$, indicating that
$S_{\mathcal{M},\mathbf{g},D}$ is so difficult to shrink that its
size is greater than our expected upper bound.

\subsection{A cost learning objective}

\bmcomment{Cite self-paced learning for inspiration for new objective function?}

\bmcomment{Add footnote about relationship between norm in objective and
Mahalanobis norm}

\bmcomment{Is there any math I should be more explicit about?}

We can modify the margin re-scaling SVM learning procedure given by 
quadratic program \ref{marginRescaling} to learn the cost function 
according to easiness measure \ref{easiness}.
First, we transform the quadratic program
into the equivalent unconstrained optimization problem:

\begin{equation}
\label{svmObjective}
\min_{\mathbf{w}} \frac{\lambda_2}{2}\|\mathbf{w}\|_2^2 + \sum_{i=1}^m\bigg(-F(\mathbf{x}_i,\mathbf{y}_i;\mathbf{w})+\max_{\mathbf{y}\in \mathcal{Y}}\Big(F(\mathbf{x}_i,\mathbf{y};\mathbf{w})+\Delta(\mathbf{y}_i,\mathbf{y})\Big)\bigg)
\end{equation}

And we modify this function to include the cost learning as:

\begin{equation}
\label{costObjective}
\min_{\mathbf{\mathcal{\hat{E}}}\geq 0,\mathbf{w}} \frac{\lambda_2}{2}\|\mathbf{w}\|_2^2 + \sum_{i=1}^m\bigg(-F(\mathbf{x}_i,\mathbf{y}_i;\mathbf{w})+\max_{\mathbf{y}\in \mathcal{Y}}\Big(F(\mathbf{x}_i,\mathbf{y};\mathbf{w})+\mathbf{\mathcal{\hat{E}}}^\top \mathbf{\mathcal{S}}(\mathbf{y}_i,\mathbf{y})\Big)\bigg)-\mathbf{\mathcal{\hat{E}}}^\top\mathbf{n}+\frac{1}{2}\|\mathbf{\mathcal{\hat{E}}}\|^2_\mathbf{n}
\end{equation}

Where $\mathbf{n}$ is the vector of normalization constants, and 
$\|\mathbf{\mathcal{\hat{E}}}\|^2_n=\sum_{S\in\mathcal{S}}n_S\mathcal{\hat{E}}_S^2$.\footnote{
  We abbreviate $\mathcal{\hat{E}}(S,\mathcal{M},\mathbf{g},D)$ and 
 $n_{S,\mathcal{M},\mathbf{g},D}$ as $\mathcal{\hat{E}}_S$ and $n_S$ for 
 readability.
}
The idea for this new objective function is to use the 
$-\mathbf{\mathcal{\hat{E}}}^\top\mathbf{n}$ term to select which 
$\mathcal{\hat{E}}_S$
should be non-zero (or correspondingly which $S$ are not 
impossibly difficult), and use the $\|\mathcal{\hat{E}}\|^2_\mathbf{n}$ 
to limit the magnitude of $\mathcal{\hat{E}}_S$.

If the solution to objective \ref{costObjective} is at point that is 
differentiable with respect to $\mathcal{\hat{E}}_S$, then it's easy to 
show that $\mathcal{\hat{E}}_S$ has exactly the value given by 
equation \ref{easiness}.
Otherwise, if the solution is at a non-differentiable point, then 
$\mathcal{\hat{E}}_S$ has a value that approximates equation \ref{easiness}
in a sensible way.  

\bmcomment{Fill in details about non-differentiable solutions. I'm thinking
we can put in detail about this if there's space, but otherwise just mention
it quickly?}

\subsection{Some 'easiness' normalization constants}

\bmcomment{Mention choosing n using baseline 
SVM especially if this is included in results.  But otherwise
maybe put it in future work since choosing n based on other
models is a general topic to explore}

The appropriate choice for the normalization vector $\mathbf{n}$ in 
objective \ref{costObjective} depends on the prediction classes
$\mathcal{S}$ and the type of irrelevant bias we are trying to
remove from 'easiness' measure. For $\mathcal{S}_{[\mathcal{Y}]^2}$
and $\mathcal{S}_{\mathcal{Y}^2}$, we want to remove the bias introduced
into the size of the prediction classes by non-uniform label 
distributions, as discussed at the beginning of section
\ref{costLearningModel}.  Otherwise, the model will tend to over-estimate
the costs of incorrect predictions involving labels that occur 
infrequently in the training data.  The following are two 
plausible choices of $\mathbf{n}$ to achieve this goal. In each,
assume that $D_\mathbf{y}$ is the set of training examples
for which $\mathbf{y}$ is the correct label.

\begin{enumerate}

\item \textbf{Logical} $\mathbf{n}$: Choose $n_S$ as an upper bound
on $|S|$ that cannot possibly be violated given the training data. 
For prediction classes 
$S_{\mathbf{y},\mathbf{y'}}\in\mathcal{S}_{\mathcal{Y}^2}$, 
choose $n_{S_{\mathbf{y},\mathbf{y'}}}=|D_\mathbf{y}|$, and for prediction 
classes 
$S_{\{\mathbf{y},\mathbf{y'}\}}\in\mathcal{S}_{[\mathcal{Y}]^2}$,
choose 
$n_{S_{\{\mathbf{y},\mathbf{y'}\}}}=
\max\big(|D_\mathbf{y}|,|D_\mathbf{y'}|\big)$.

\item \textbf{Expected} $\mathbf{n}$:  Choose $n_S$ as the expected
number of times a model makes a mistake in $S$ if it predicts labels
at random according to their distribution in the training data. 
For prediction classes 
$S_{\mathbf{y},\mathbf{y'}}\in\mathcal{S}_{\mathcal{Y}^2}$, 
choose $n_{S_{\mathbf{y},\mathbf{y'}}}=
\frac{|D_\mathbf{y}||D_\mathbf{y'}|}{m}$, 
and for prediction classes 
$S_{\{\mathbf{y},\mathbf{y'}\}}\in\mathcal{S}_{[\mathcal{Y}]^2}$,
choose 
$n_{S_{\{\mathbf{y},\mathbf{y'}\}}}=
\frac{2|D_\mathbf{y}||D_\mathbf{y'}|}{m}$.
Other variations of this idea might choose alternative models by which
to compute the expectation, but model that makes predictions at
according to the training data label distribution seems 
reasonable for getting rid of the label distribution bias.

\end{enumerate}

In general, the \textbf{Logical} choice of $\mathbf{n}$ is an upper
bound on the \textbf{Expected} choice.  The \textbf{Logical} choice
will tend to over-estimate the maximum size of each prediction class,
but we might choose it over \textbf{Expected} if we have reason
to believe that it is difficult to estimate the baseline rate at
which the model is biased to predict certain labels by the label
distribution independent of the inputs.

\bmcomment{More detail on why Expected might be a better 
choice than Logical}

\section{Experiments}

\bmcomment{Cite Pegasos for SGD learning rate}

\bmcomment{Add graphs showing convergence of SGD?}

We implemented the multiclass SVM and normalized cost learning SVM (SVMCLN) 
using stochastic gradient descent (SGD) to approximate objectives
\ref{svmObjective} and \ref{costObjective}.  Our SGD implementation
used a learning rate of $\frac{1}{\lambda_2 t}$ at time-step $t$.
We ran several experiments to compare these models on standard
text classification corpora.  In each experiment, we compared 
the accuracies of the models on standard train/test split 
given by each respective
data corpus while also randomly selecting a 10\% subset of the 
training data to use as a dev set.  We used the dev set to perform
a grid search over 16 possible values ranging from $10^{-6}$ to 
$0.5\times 10^2$ for the $\lambda_2$ hyper-parameter in each model.
After the grid search, we fixed the best value for $\lambda_2$,
and retrained on the full training data,
evaluating the final performance of the model on the test set.  
 
Due to the stochasticity of the optimization algorithm, we 
expected some noisiness in its convergence.  In general, we 
ran SGD for 150 iterations over the random permutations
of the full data. After the 150 iterations, the accuracy and 
predictions of each model on the dev/test sets were relatively 
stable; the accuracy varied by at most $~0.001$ and 
fewer $5\%$ of the predictions changed 
during the last 10 iterations over the training 
data. 

\subsection{Datasets}

\bmcomment{Cite \url{http://www.aaai.org/Papers/AAAI/2006/AAAI06-121.pdf}
on relatively low difference in performance between log(tf) and tfidf}

\bmcomment{Cite \url{http://qwone.com/~jason/writing/loocv.pdf} for 
example use of log(tf)}

\bmcomment{Preprocessed using method from 
\url{http://web.ist.utl.pt/acardoso/docs/2007-phd-thesis.pdf}}

We compared the performance of the models on the 20 Newsgroups and
the Reuters-21578 (R52) data sets.  We chose these two data sets
because they both have a relatively large number of output labels,
some of which might not be distinctive enough for the SVM to plausibly
learn well given its features, in which case we might expect a gain in 
accuracy from SVMCLN.  As shown below, we found out that this 
expectation was correct for the 20 Newsgroups data, but incorrect for 
the Reuters data, for reasons that became apparent to us after we
reviewed the results.

For both of these data sets, the target labels $\mathcal{Y}$ are 
single document topics/categories, and the inputs $\mathcal{X}$ are
documents.  We preprocessed each text document from both corpora 
using the procedure given in [FILL IN] (convert to lower-case, 
remove symbols, etc), and then we computed $\mathbf{g}$ as normalized
$\log$ unigram term-frequency features.  In particular:

\begin{equation}
\mathbf{g}_{\mathbf{y'}}(\mathbf{x},\mathbf{y}) = \mathbbm{1}(\mathbf{y}=\mathbf{y'})\mathbf{f}(\mathbf{x})
\end{equation}

Where $\mathbf{f}(\mathbf{x})$ is a normalized vector whose elements 
are $\log(1+tf(\mathbf{x},v))$ when $v$ is a unigram in the corpus 
vocabulary and $tf(\mathbf{x},v)$ is the frequency of $v$ in document
 $\mathbf{x}$.

\subsubsection{Reuters 21578 (R52)}

The Reuters-21578 R52 corpus contains 9100 documents from the original
Reuters-21578 data, each of which is labeled with one of 52 possible topics.
\footnote{See 
\url{http://www.csmining.org/index.php/r52-and-r8-of-reuters-21578.html}.}  
The documents are divided along the Reuters-21578 'ModApte' split
into \~70\% training and \~30\% test.  The label distribution is extremely
non-uniform, with 43\% of the documents assigned to a single topic, and
71\% of the other topics each assigned fewer than 50 (0.5\%) documents.

\subsubsection{20 newsgroups}

The 'by date' version of the 20 Newsgroups data contains 
18846 documents sorted by date into 60\% for training and 40\% for testing.  
\footnote{See \url{http://qwone.com/~jason/20Newsgroups/}.} In comparison
to the Reuters data, the newsgroups distribution of 20 categories is
nearly uniform, with some topics highly related and some topics entirely
unrelated.  The relatedness of the topics is approximately captured by
their hierarchical naming scheme (e.g. there is \textit{rec.autos} and
\textit{rec.motorcycles} which are likely to be highly related to each
other, but mostly unrelated to \textit{sci.space}).  We expected that
this hierarchy would be encoded by the learned 'easiness' approximations,
since the 'easiness' with which the model discriminates two categories
likely decreases with their relatedness.

\subsection{Results}

\bmcomment{All 20news table numbers are wrong.  Need to fix them after experiments rerun.}

Table~\ref{accuraciesTable} shows the micro-averaged
accuracies on the Reuters and 20 newsgroups tasks for the 
SVM baseline model and versions of SVMCLN with
different choices of normalization constants 
$\mathbf{n}$ and incorrect prediction classes $\mathcal{S}$.  The last column 
in the table shows that none of the SVMCLN variations improve the
 ccuracy over the SVM on the Reuters task, but inspection of the 
confusion matrix for the SVM
baseline provides some intuition for this lack of improvement.  
The matrix shows
that $53\%$ of the SVM's mistakes were on examples with whose topics had 10 or
fewer test examples, all off which were predicted incorrectly.  Furthermore,
there is no non-diagonal element in the confusion matrix with more than 10 
mistakes.  These observations suggest that many of the SVM baseline's mistakes
come from infrequent labels rather than systematic conflations between certain
label pairs, and further that all of the incorrect prediction classes in the
cost learning model will be extremely small to begin with.  As a result, it's
unlikely that SVMCLN would be able to re-scale the costs to greatly shrink
any incorrect prediction classes.  In hindsight, this provides a good example
of a good example of how analysis of the errors made by the standard SVM can
determine whether cost learning will be beneficial.

In contrast, the fourth column of Table~\ref{accuraciesTable} shows that 
every SVMCLN variation improves the accuracy over the SVM on the 20 Newsgroups 
data.  Unsurprisingly, the \textbf{Logical} 
and \textbf{Expected} normalized versions
perform better than the non-normalized versions since they should give better
estimates of the 'easiness' measure and cost function.  Also, each 
$\mathcal{S}_{[\mathcal{Y}]^2}$ version performs slightly better than 
each $\mathcal{S}_{[\mathcal{Y}]^2}$ version, which makes sense given that
the ease of resolving mistakes in $S_{\mathbf{y},\mathbf{y}'}$ should be 
the same as the ease of resolving mistakes in $S_{\mathbf{y}',\mathbf{y}}$,
and so the ordering on $\mathbf{y}$ and $\mathbf{y}'$ gives the model
unnecessary extra parameters.  We also expected 
\textbf{Expected} to perform better 
than \textbf{Logical} since \textbf{Logical} 
over-estimates the maximum value of each incorrect prediction class, but
this expectation was not met with 
$\mathcal{S}_{[\mathcal{Y}]^2}$ prediction
classes, possibly because the \textbf{Expected} normalizers tend 
to under-estimate.

\begin{table}[t]
\caption{Micro-averaged Accuracies}
\label{accuraciesTable}
\begin{center}
\begin{tabular}{lllcccc}
\bf{Model}  & \bf{$\mathbf{n}$} & \bf{$\mathcal{S}$} & \bf{20news} & \bf{20news level 2}   & \bf{20news level 1}   & \bf{Reuters}
\\ \hline \\
SVM         & N/A               & N/A                & 0.7760      & 0.8008                & 0.8654                & 0.9213 \\
SVMCLN      & None              & $\mathcal{Y}^2$    & 0.8008      & 0.8259                & 0.8752                & 0.9213 \\ 
SVMCLN      & None              & $[\mathcal{Y}]^2$  & 0.8011      & 0.8257                & 0.8751                & 0.9194 \\
SVMCLN      & Logical           & $\mathcal{Y}^2$    & 0.8024      & 0.8271                & 0.8769                & 0.9210 \\ 
SVMCLN      & Logical           & $[\mathcal{Y}]^2$  & 0.8376      & 0.8631                & 0.9142                & 0.9159 \\
SVMCLN      & Expected          & $\mathcal{Y}^2$    & 0.8303      & 0.8557                & 0.9092                & 0.9159 \\ 
SVMCLN      & Expected          & $[\mathcal{Y}]^2$  & 0.8307      & 0.8558                & 0.9084                & 0.9171 \\
\end{tabular}
\end{center}
\end{table}

The hierarchical structure of the 20 Newsgroups topics encodes a notion
of distance between topics as distance within the hierarchy.  The fifth
and sixth columns of Table~\ref{accuraciesTable} show the accuracies 
computed when nearby topics in the hierarchy are are collapsed into
single topics--the fifth column shows the accuracies computed when all
topics that are the same to two levels deep in the hierarchy are collapsed
into a single topic, and the sixth column shows the accuracies computed
when all topics that are the same at the first level of the hierarchy are
collapsed into a single topic.  

\bmcomment{I think there are ways to make the following result clearer
if we compute other numbers... but I don't know if we have time for that.
This may be a bit confusing I think though}

The cost learning should learn a notion of distance between topics 
approximated by the cost function, and we expect this notion of distance
to approximate the notion of distance encoded by the hierarchy.  The 
approximated notion of distance encoded in the cost function should 
cause SVMCLN to improve at distinguishing topics that are far apart
from each other by the notion of distance approximated by the cost 
function.  So if SVMCLN's learned distances approximate the distances
through the hierarchy, then we should expect the SVMCLN's collapsed 
accuracy improvements to be nearly as great as the uncollapsed accuracy
improvements.  This is shown by the [FILL] accuracy improvements in the
fourth, fifth, and sixth columns of Table~\ref{accuraciesTable} by
the \textbf{Logical} $\mathcal{S}_{[\mathcal{Y}]^2}$ version of SVMCLN.

\bmcomment{Fix the above paragraph when get new numbers}

% Learned hierarchy

%\begin{tikzpicture}
%\tikzset{grow'=right,level distance=32pt}
%\tikzset{execute at begin node=\strut}
%\tikzset{every tree node/.style={anchor=base west}}
%
%\Tree [
%[ the ]
%[ cat ]
%]
%\end{tikzpicture}
%
%\begin{tikzpicture}[sloped]
%\node (a) at (-6,0) {a};
%\node (b) at (-3,0) {b};
%\node (c) at (-0.5,0) {c};
%\node (d) at (0.5,0) {d};
%\node (e) at (2,0) {e};
%\node (ab) at (-4.5,3) {};
%\node (cd) at (0,1) {};
%\node (cde) at (1,2) {};
%\node (all) at (-1.5,5) {};
% 
%\draw  (a) |- (ab.center);
%\draw  (b) |- (ab.center);
%\draw  (c) |- (cd.center);
%\draw  (d) |- (cd.center);
%\draw  (e) |- (cde.center);
%\draw  (cd.center) |- (cde.center);
%\draw  (ab.center) |- (all.center);
%\draw  (cde.center) |- (all.center);
% 
%\draw[->,-triangle 60] (-7,0) -- node[above]{distance} (-7,6);
%\end{tikzpicture}


\section{Discussion}

\subsection{Related literature}

% See possible reference list below
\bmcomment{I have trouble writing this part.}

\subsection{Future work}

\bmcomment{There's probably lots of stuff I don't know about
which could go here.  Here are some ideas though:}

\bmcomment{Incorporate input features into cost}

\bmcomment{Other choices for n}

\bmcomment{Other measures of 'easiness'}

\bmcomment{Structured version}

\bmcomment{Alternating minimization version}

\bmcomment{Relationship between model 'difficulty' and 'task' difficulty}

\bmcomment{Characterize properties of data/features that determine whether cost learning is useful}

%\subsubsection*{Acknowledgments}

%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments go at the end of the paper. Do not include 
%acknowledgments in the anonymized submission, only in the 
%final paper. 

\bmcomment{The 'references' heading given by the 
bibliography command is the wrong size font.  Needs to
be the size of a 'third level heading'.  How to change this?}

\bibliography{refs}

% Here's how things are formatted in the NIPS template:
%
%
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.

%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}

\end{document}
