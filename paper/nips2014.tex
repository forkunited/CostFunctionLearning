\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\newcommand{\bmcomment}[1]{\textcolor{green}{\textsc{\textbf{[#1 --wvm]}}}}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Learning when to try hard at learning}

\author{
Bill McDowell\thanks{Alternative email: \texttt{forkunited@gmail.com}}
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{wmcdowel@cs.cmu.edu} \\
\And
Noah A.~Smith
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{nasmith@cs.cmu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\bmcomment{Probably need a better title.}

\bmcomment{Noah, I put your name second because your name seems 
to be last on all of your recent papers.  I don't care which our names is
first though, so feel free to swap if you want.}

\begin{abstract}
% Fill me in later
\end{abstract}

\section{Introduction}

\section{Background}

\bmcomment{Cite some paper on using binary svms}
\bmcomment{Cite multiclass svm}
\bmcomment{Cite structured svm}

For the multiclass classification problem, we are given a set of
$m$ labelled training data
$D=\{(\mathbf{x}_1,\mathbf{y}_1),...,(\mathbf{x}_m, \mathbf{y}_m)\}$ where each
example $\mathbf{x}_i\in \mathcal{X}$ is assigned label 
$\mathbf{y}_i\in\mathcal{Y}$, and we want to learn a function 
$f:\mathcal{X}\rightarrow\mathcal{Y}$ which gives the correct
label for any input taken from $\mathcal{X}$.  Past work has developed
a multiclass support vector machine (SVM) which generalizes ideas
employed by SVMs in the binary classification domain.  Given that
$\mathbf{g}:\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}^k$ 
computes a featurized representation of an input-output pair and
$\mathbf{l}:\mathcal{Y}\rightarrow\{0,1\}^{|\mathcal{Y}|}$ a 
vector of indicators representing an output label, the SVM makes 
predictions according to the function:

\begin{equation}
\label{prediction}
f(\mathbf{\mathbf{x};\mathbf{w}})=\argmax_{\mathbf{y}'\in\mathcal{Y}}\Big( \mathbf{w}^\top \mathbf{g}(\mathbf{x},\mathbf{y}')+\mathbf{b}^\top\mathbf{l}(\mathbf{y}') \Big)
\end{equation}

Where the feature weights $\mathbf{w}$ and label biases $\mathbf{b}$
are given by the solution to the following optimization problem:

\begin{equation}
\begin{split}
\label{objective}
& \min_{\mathbf{w}, \mathbf{b}} \lambda_2\|\mathbf{w}\|_2^2+ \\
 & +\sum_{i=1}^N\bigg(-\mathbf{w}^\top \mathbf{g}(\mathbf{x}_i,\mathbf{y}_i)-\mathbf{b}^\top \mathbf{l}(\mathbf{y}_i)+\max_{\mathbf{y}\in \mathcal{Y}}\Big(\mathbf{w}^\top\mathbf{g}(\mathbf{x}_i,\mathbf{y})+\mathbf{b}^\top \mathbf{l}(\mathbf{y})+\Delta(\mathbf{y}_i,\mathbf{y})\Big)\bigg)
\end{split}
\end{equation}

In general, we refer to the $\Delta$ as a cost function, where in the above SVM optimization problem, the cost 
$\Delta(\mathbf{y}_i,\mathbf{y})=\mathbbm{1}(\mathbf{y}_i\neq\mathbf{y})$ 
indicates whether $\mathbf{y}_i$ is equal to $\mathbf{y}$.  This optimization
maximizes the 
 

-Introduce data, x,y and multi-label classification.
-Several binary SVMs have been used in the past to do multiclass
-Crammer and Singer introduce single multiclass classifier
-Structured SVM has cost that varies depending on structure


%- SVM have a cost function
%- Want to learn this
%- Explain intuition
%- Desired properties of the cost function

\section{Cost Learning Model}

%- With/without n
%- Intuition
%- Properties
%- Values of n

\section{Experiments}
%- Model implementation
%- Pegasos
%- Data 
%- 20newsgroups
%- Reuters
%- Performance
%- Learned cost weights, relation to hierarchies

\section{Discussion}

\subsection{Related Literature}

% See possible reference list below

\subsection{Future Work}

% Incorporate input features into cost
% Other choices for n
% Other measures of 'difficulty'
% Structured version
% Relationship between model 'difficulty' and 'task' difficulty
% Characterize properties of data/features that determine whether cost learning is useful

%\subsubsection*{Acknowledgments}

%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments go at the end of the paper. Do not include 
%acknowledgments in the anonymized submission, only in the 
%final paper. 

\subsubsection*{References}
% Here are some references to possibly include
%
% TODO: Things about convexity
%
% Structured Perceptron
% Collins, Michael. ``Discriminative training methods for 
% hidden markov models: Theory and experiments with perceptron algorithms.'' In Proceedings 
% of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pp. 1-8. Association for Computational Linguistics, 2002.
% 
% Structured SVM
% Tsochantaridis, Ioannis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun, and Yoram Singer. 
% ``Large margin methods for structured and interdependent output variables.'' Journal of Machine Learning Research 6, no. 9 (2005).
%
% Multiclass SVM
% Crammer, Koby, and Yoram Singer. 
% ``On the algorithmic implementation of multiclass kernel-based vector machines.'' The Journal of Machine Learning Research 2 (2002): 265-292.
%
% Also Multiclass SVM
% Weston, Jason, and Chris Watkins. Multi-class support vector machines. Technical Report CSD-TR-98-04, Department of Computer Science, 
% Royal Holloway, University of London, May, 1998.
%
% TODO:20newsgroups 
% 
% TODO:Reuters
%
% SGD algorithm implementation/learning rate
% Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew Cotter. 
% ``Pegasos: Primal estimated sub-gradient solver for svm.'' Mathematical programming 127, no. 1 (2011): 3-30.
%
% Inspiration for objective function (select cost weights to turn on)
% Kumar, M. Pawan, Benjamin Packer, and Daphne Koller. ``Self-Paced Learning for Latent Variable Models.'' In NIPS, vol. 1, p. 3. 2010.
%
% Curriculum Learning
% Bengio, Yoshua, JÃ©rÃŽme Louradour, Ronan Collobert, and Jason Weston
%. ``Curriculum learning.'' In Proceedings of the 26th annual international conference on machine learning, pp. 41-48. ACM, 2009.
%
% Confident weighted learning
% Dredze, Mark, Koby Crammer, and Fernando Pereira. ``Confidence-weighted linear classification.'' In Proceedings 
% of the 25th international conference on Machine learning, pp. 264-271. ACM, 2008.
%
% Some other things from http://www.cs.cmu.edu/~nasmith/papers/career-proposal-2010.pdf 
%   - Hidden variable learning by state splitting?
%   - Finite state output encodings
%
% Here's how things are formatted in the NIPS template:
%
%
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.

%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}

\end{document}
