\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage[sort&compress]{natbib}
\usepackage{natbibspacing}

\usetikzlibrary{arrows}
\usetikzlibrary{shapes, trees}

\bibliographystyle{unsrt}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\unorderedS}{\mathcal{S}^{\mathrm{u}}}
\newcommand{\orderedS}{\mathcal{S}^{\mathrm{o}}}

\newcommand{\bmcomment}[1]{\textcolor{blue}{\textsc{\textbf{[#1 --bm]}}}}
\newcommand{\nascomment}[1]{\textcolor{red}{\textsc{\textbf{[#1 --nas]}}}}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Learning Not to Try Too Hard}

\author{
Bill McDowell\thanks{Alternative email: \texttt{forkunited@gmail.com}}
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{wmcdowel@cs.cmu.edu} \\
\And
Noah A.~Smith
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{nasmith@cs.cmu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle


\bmcomment{It seems like a lot of other papers refer to 'cost' as 'loss'... 
Is there somewhere where people call it 'cost' like we are?}
\nascomment{will address this}

\bmcomment{Noah, I put your name second because your name seems 
to be last on all of your recent papers.  I don't care which our names is
first though, so feel free to swap if you want.} \nascomment{I usually
go last}

\begin{abstract}
\nascomment{do last}
\end{abstract}

\section{Introduction}

Discriminative learning algorithms are often motivated by their
ability to trade off among different kinds of prediction mistakes with
different costs.  The cost of a mistake is usually taken to be fully
defined by the task, i.e., human system designers are trusted to
encode this knowledge prior to learning.  Information about the
inherent ease of avoiding some errors vs.~others is generally not
taken into account.   Closely related to this, and critically
important in domains where the data is constructed by humans, is the
problem that the outputs in the training data may be unreliable.  For
example, if two labels are ill-defined by the data-generating process,
then a learner can be forgiven for conflating them.

We consider situations where human intuition about relative costs of
different errors is insufficient.  In a margin-based linear modeling
framework, we propose a method for incorporating \textbf{learning of
  the cost function} alongside learning of the model.  Our approach
introduces explicit estimates of the ``ease'' of avoiding each type of
error
(for a particular model family).   For error types that are ``just too
hard,'' our model is offered the possibility of giving up in favor of
making other, less challenging predictions more accurately




\bmcomment{Cite example of unreliable output labels}

\bmcomment{Might want to give some motivating examples? Not sure if that's
necessary or not. One example of multiclass domain, and one example where
cost functions are used in structured domains}

\bmcomment{Mention examples of measures of 'difficulty'? Cite.}



Our experiments show benefits on standard benchmarks
\nascomment{change if only one} in text classification.


\section{Background and Notation}

In a prediction problem, let $\mathcal{X}$ denote the input space,
$\mathcal{Y}$ denote the output space, and assume $N$ training
instances $\{(x_1, y_1), \ldots, (x_N, y_N)\}$.  We assume a linear
model and prediction function:
\begin{equation}
\hat{y} = \argmax_{y \in \mathcal{Y}} \left(f(x, y;\mathbf{w}) \triangleq \mathbf{w}^\top \mathbf{g}(x, y) \right)
\end{equation}
where $\mathbf{w} \in \mathbb{R}^D$ are the parameters to be learned
and $\mathbf{g} : \mathcal{X} \times \mathcal{Y} \rightarrow
\mathbb{R}^D$ is the feature vector function.  We will let
$\mathcal{M} =\{f(\cdot,\cdot;\mathbf{w}) \mid \mathbf{w} \in
\mathbb{R}^D\}$ denote the model family under consideration, given a
fixed choice of $\mathbf{g}$.

Our approach, which assumes $\mathcal{Y}$ is categorical, is based on
the soft margin formulation of multiclass support vector machines
\citep{vapnik1998statistical,crammer2002algorithmic,weston1998multi}.
Tsochantaridis et al.~\citep{tsochantaridis2004support} and Taskar et
al.~\ref{koller2003max} generalized this framework to allow for
differences in costs between different kinds of mistakes, as found
when $\mathcal{Y}$ is structured.  Let the cost function
$\Delta:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}$  be such
that $\Delta(y, y')$ is the cost of predicting $y$ when the correct
label is $y'$.  We use the ``margin rescaling'' variant the multiclass SVM:
\begin{align}
 \min_{\boldsymbol{\xi}\geq 0, \mathbf{w}}
\frac{\lambda}{2}\|\mathbf{w}\|_2^2+\frac{1}{m}\sum_{i=1}^m\xi_i 
 \text{\ \ \ s.t.\ \ \ } \forall i,  \forall y \in \mathcal{Y} \setminus
 \{y_i\},  f(x_i, y_i; \mathbf{w}) - f(x, y, \mathbf{w}) \geq
 \Delta(y, y_i) - \xi_i \label{marginRescaling}
\end{align}
This objective seeks $\mathbf{w}$ that minimizes misclassifications while
maximizing the margin between correct and incorrect instances.
Further, the more incorrect an $(x,y)$ pair is, the greater the margin
should be.

Previous work assumes $\Delta$ follows intuitively from the prediction
task.  For example, in natural language dependency parsing, the number
of words attached to the wrong parent (Hamming distance for the parse
tree) is a sensible choice.  

We propose to parameterize $\Delta$ and
learn its parameters jointly with $\mathbf{w}$.  This learned cost
function should encode distances between outputs from the perspective
of the ease with which a model in the family $\mathcal{M}$
can distinguish between them.
This joint learning setup is expected to be particularly useful when
some classes of errors are difficult or impossible for a model in the
class to resolve, due to unreliable annotations or an insufficient
choice of features $\mathbf{g}$.

We let $\mathcal{S} \subseteq 2^{\mathcal{Y}\times\mathcal{Y}}$ be a
collection of prediction error classes that exhausts $\mathcal{Y}^2$ (i.e., $\bigcup_{S \in
  \mathcal{S}} S = \mathcal{Y}^2$); the error classes need not be
mutually exclusive.  We let $e_{S} \in \mathbb{R}$ denote an estimate
of the ``ease'' with which any model in the linear family (given by
$\mathbf{g}$) can learn to avoid errors in class $S$.  Then we let:
\begin{equation}
\Delta(y, y') = \sum_{S \in \mathcal{S}: (y, y') \in S} e_S =
\mathbf{e}^\top \mathbf{s}(y, y')
\end{equation}
where $\mathbf{e}$ is a vector of the $e_S$ and $\mathbf{s}$ is a
binary vector of length $\mathcal{S}$ indicating which error class(es)
each possible confusion belongs to.

In this paper, we consider two prediction error classes, corresponding
to unordered and ordered pairs of outputs.  We denote them
$\unorderedS$ and $\orderedS$, respectively.

% There are many possible choices for prediction classes $\mathcal{S}$, but
% in this paper, we focus on the following two:

% \begin{equation}
% \begin{split}
% & \mathcal{S}_{[\mathcal{Y}]^2} = \{ S_{\{\mathbf{y},\mathbf{y}'\}} | \mathbf{y},\mathbf{y}'\in\mathcal{Y}\} \\
% \text{ where } & S_{\{\mathbf{y},\mathbf{y}'\}}=
% \{ (\mathbf{y}_1, \mathbf{y}_2) | (\mathbf{y}_1=\mathbf{y}\wedge \mathbf{y}_2=\mathbf{y}')\vee(\mathbf{y}_1=\mathbf{y}'\wedge \mathbf{y}_2=\mathbf{y}) \} \\
% \end{split}
% \end{equation}

% \begin{equation}
% \mathcal{S}_{\mathcal{Y}^2} = \{ S_{(\mathbf{y},\mathbf{y}')} | \mathbf{y},\mathbf{y}'\in\mathcal{Y}\}\text{ where }S_{(\mathbf{y},\mathbf{y}')}=\{ (\mathbf{y},\mathbf{y}') \} \\
% \end{equation}

% $\mathcal{S}_{[\mathcal{Y}]^2}$ contains a prediction class for every
% unordered pair of labels, and $\mathcal{S}_{\mathcal{Y}^2}$ contains
% a prediction class for every ordered pair.  We might choose 
% $\mathcal{S}_{[\mathcal{Y}]^2}$ when factoring the cost function if
% we believe it is equally easy to resolve misclassifications of 
% $\mathbf{y}$ as $\mathbf{y}'$ and misclassifications of $\mathbf{y}'$
% as $\mathbf{y}$.  Otherwise, if we suspect these two incorrect 
% prediction types to differ in easiness, then we might choose 
% $\mathcal{S}_{\mathcal{Y}^2}$.

\section{Cost Learning Model}
\label{costLearningModel}

We desire a model that estimates prediction ease $\mathbf{e}$ while
estimating
predictive model parameters $\mathbf{w}$.  Above, we defined ``ease''
with respect to an arbitrary model in the family $\mathcal{M}$, but it
is more sensible to consider the particular model we seek to
estimate.  We propose that, for error class $S$ and a model with
parameters $\mathbf{w}$, ease $e_S$ should be proportional to the rate
of margin violations involving $S$ that $f(\cdot, \cdot; \mathbf{w})$ makes in the
training data:
\begin{equation}
\left\{ i  \in \{1,\ldots, D\} \mid \left(y_i, \textstyle \argmax_{y \in \mathcal{Y}} f(x_i, y ; \mathbf{w}) +
\mathbf{e}^\top \mathbf{s}(y, y_i)\right) \in S\right\}
\end{equation}

\nascomment{left off here}

The size of $S_{\mathcal{M},\mathbf{g},D}$ is the number of
training examples with margin violations in class $S$. If
the size of this set is large, we might infer that the
model has trouble shrinking it, and so it's not 'easy'.  This 
might lead us to conclude that $S_{\mathcal{M},\mathbf{g},D}$ 
tends to shrink with 'easiness'.  However,
for many data sets and choices of $\mathcal{S}$, the size 
of each $S_{\mathcal{M},\mathbf{g},D}$ can be inherently
biased by the data independently of 'easiness'.  For example, 
for $S_{\{\mathbf{y},\mathbf{y}'\}}\in\mathcal{S}_{[\mathcal{Y}]^2}$, 
the size of $S_{\{\mathbf{y},\mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ 
is biased by the number of examples in the training data which have 
output labels $\mathbf{y}$ and $\mathbf{y}'$--if there are few 
training examples of labels 
$\mathbf{y},\mathbf{y}'\in\mathcal{Y}$, then the size of 
$S_{\{\mathbf{y}, \mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ 
will necessarily be small relative to other prediction classes.
Furthermore, we expect 
output labels which occur infrequently in the training data to 
be more difficult for the model to predict correctly, so this
will lead to the size of 
$S_{\{\mathbf{y},\mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ 
increasing with the 'easiness' of 
$S_{\{\mathbf{y},\mathbf{y}'\}}$ which is opposite the 
conclusion that that we draw if we think of the size of 
$S_{\{\mathbf{y},\mathbf{y}'\},\mathcal{M},\mathbf{g},D}$ as
increasing due to the model's difficulty in shrinking it.  In general, 
this suggests that if we want the size of $S_{\mathcal{M},\mathbf{g},D}$
to vary with easiness, we need to normalize it to account for 
properties of the training data that introduce irrelevant
biases.  

\subsection{A measure of 'easiness'}

\bmcomment{Might want to discuss other possibilities, or generally
refer to other possibilities in future work}

The above observations suggest the following as a possible 
measure of 'easiness':

\begin{equation}
\label{easiness}
\mathcal{E}(S,\mathcal{M},\mathbf{g},D)=\max\bigg(0, 1-\frac{|S_{\mathcal{M},\mathbf{g},D}|}{n_{S,\mathcal{M},\mathbf{g},D}}\bigg)
\end{equation}

Where $n_{S,\mathcal{M},\mathbf{g},D}$ is a normalization constant which
gives the maximum possible value we expect for the size of 
$S_{\mathcal{M},\mathbf{g},D}$, accounting for irrelevant biases
introduced by the data as discussed above.  This measure of easiness 
is in $[0,1]$, and it has the property that if 
$|S_{\mathcal{M},\mathbf{g},D}|\geq n_{S,\mathcal{M},\mathbf{g},D}$,
then $\mathcal{E}(S,\mathcal{M},\mathbf{g},D)=0$, indicating that
$S_{\mathcal{M},\mathbf{g},D}$ is so difficult to shrink that its
size is greater than our expected upper bound.

\subsection{A cost learning objective}

\bmcomment{Cite self-paced learning for inspiration for new objective function?}

\bmcomment{Add footnote about relationship between norm in objective and
Mahalanobis norm}

\bmcomment{Is there any math I should be more explicit about?}

We can modify the margin re-scaling SVM learning procedure given by 
Quadratic Program~\ref{marginRescaling} to learn the cost function 
according to the easiness measure shown in Equation~\ref{easiness}.
First, we transform the quadratic program
into the equivalent unconstrained optimization problem:

\begin{equation}
\label{svmObjective}
\min_{\mathbf{w}} \frac{\lambda_2}{2}\|\mathbf{w}\|_2^2 + \sum_{i=1}^m\bigg(-F(\mathbf{x}_i,\mathbf{y}_i;\mathbf{w})+\max_{\mathbf{y}\in \mathcal{Y}}\Big(F(\mathbf{x}_i,\mathbf{y};\mathbf{w})+\Delta(\mathbf{y}_i,\mathbf{y})\Big)\bigg)
\end{equation}

And we modify this function to include the cost learning as:

\begin{equation}
\label{costObjective}
\min_{\mathbf{\mathcal{\hat{E}}}\geq 0,\mathbf{w}} \frac{\lambda_2}{2}\|\mathbf{w}\|_2^2 + \sum_{i=1}^m\bigg(-F(\mathbf{x}_i,\mathbf{y}_i;\mathbf{w})+\max_{\mathbf{y}\in \mathcal{Y}}\Big(F(\mathbf{x}_i,\mathbf{y};\mathbf{w})+\mathbf{\mathcal{\hat{E}}}^\top \mathbf{\mathcal{S}}(\mathbf{y}_i,\mathbf{y})\Big)\bigg)-\mathbf{\mathcal{\hat{E}}}^\top\mathbf{n}+\frac{1}{2}\|\mathbf{\mathcal{\hat{E}}}\|^2_\mathbf{n}
\end{equation}

Where $\mathbf{n}$ is the vector of normalization constants, and 
$\|\mathbf{\mathcal{\hat{E}}}\|^2_n=\sum_{S\in\mathcal{S}}n_S\mathcal{\hat{E}}_S^2$.\footnote{
  We abbreviate $\mathcal{\hat{E}}(S,\mathcal{M},\mathbf{g},D)$ and 
 $n_{S,\mathcal{M},\mathbf{g},D}$ as $\mathcal{\hat{E}}_S$ and $n_S$ for 
 readability.
}
The idea for this new objective function is to use the 
$-\mathbf{\mathcal{\hat{E}}}^\top\mathbf{n}$ term to select which 
$\mathcal{\hat{E}}_S$
should be non-zero (or correspondingly which $S$ are not 
impossibly difficult), and use the $\|\mathcal{\hat{E}}\|^2_\mathbf{n}$ 
to limit the magnitude of $\mathcal{\hat{E}}_S$.

If the solution to Objective~\ref{costObjective} is at point that is 
differentiable with respect to $\mathcal{\hat{E}}_S$, then it's easy to 
show that $\mathcal{\hat{E}}_S$ has exactly the value given by 
Equation~\ref{easiness}.
Otherwise, if the solution is at a non-differentiable point, then 
$\mathcal{\hat{E}}_S$ has a value that approximates Equation~\ref{easiness} in a sensible way.  Specifically, the non-differentiable points of Objective~\ref{costObjective}
occur due to ties between multiple sequences of labels for the 
minimum value solution, and each of these equally good label sequences has 
differently sized incorrect prediction classes, giving different
values of 'easiness' for each class according to Equation~\ref{easiness}.
The values of $\mathcal{\hat{E}}_S$ at the non-differentiable
points are between the values given by Equation~\ref{easiness}
for each tied best label sequence.

\bmcomment{Is this non-differentiable part understandable?  Is it enough,
or do I need to give the proof?  I still haven't actually gone through
the proof completely, so maybe we should at least go over the reasoning
to make sure I'm not crazy.}

\subsection{Some 'easiness' normalization constants}

\bmcomment{Mention choosing n using baseline 
SVM especially if this is included in results.  But otherwise
maybe put it in future work since choosing n based on other
models is a general topic to explore}

The appropriate choice for the normalization vector $\mathbf{n}$ in 
Objective~\ref{costObjective} depends on the prediction classes
$\mathcal{S}$ and the type of irrelevant bias we are trying to
remove from 'easiness' measure. For $\mathcal{S}_{[\mathcal{Y}]^2}$
and $\mathcal{S}_{\mathcal{Y}^2}$, we want to remove the bias introduced
into the size of the prediction classes by non-uniform label 
distributions, as discussed at the beginning of 
Section~\ref{costLearningModel}.  Otherwise, the model will 
tend to over-estimate
the costs of incorrect predictions involving labels that occur 
infrequently in the training data.  The following are two 
plausible choices of $\mathbf{n}$ to achieve this goal. In each,
assume that $D_\mathbf{y}$ is the set of training examples
for which $\mathbf{y}$ is the correct label.

\begin{enumerate}

\item \textbf{Logical} $\mathbf{n}$: Choose $n_S$ as an upper bound
on $|S|$ that cannot possibly be violated given the training data. 
For prediction classes 
$S_{\mathbf{y},\mathbf{y'}}\in\mathcal{S}_{\mathcal{Y}^2}$, 
choose $n_{S_{\mathbf{y},\mathbf{y'}}}=|D_\mathbf{y}|$, and for prediction 
classes 
$S_{\{\mathbf{y},\mathbf{y'}\}}\in\mathcal{S}_{[\mathcal{Y}]^2}$,
choose 
$n_{S_{\{\mathbf{y},\mathbf{y'}\}}}=
\max\big(|D_\mathbf{y}|,|D_\mathbf{y'}|\big)$.

\item \textbf{Expected} $\mathbf{n}$:  Choose $n_S$ as the expected
number of times a model makes a mistake in $S$ if it predicts labels
at random according to their distribution in the training data. 
For prediction classes 
$S_{\mathbf{y},\mathbf{y'}}\in\mathcal{S}_{\mathcal{Y}^2}$, 
choose $n_{S_{\mathbf{y},\mathbf{y'}}}=
\frac{|D_\mathbf{y}||D_\mathbf{y'}|}{m}$, 
and for prediction classes 
$S_{\{\mathbf{y},\mathbf{y'}\}}\in\mathcal{S}_{[\mathcal{Y}]^2}$,
choose 
$n_{S_{\{\mathbf{y},\mathbf{y'}\}}}=
\frac{2|D_\mathbf{y}||D_\mathbf{y'}|}{m}$.
Other variations of this idea might choose alternative models by which
to compute the expectation, but model that makes predictions at
according to the training data label distribution seems 
reasonable for getting rid of the label distribution bias.

\end{enumerate}

In general, the \textbf{Logical} choice of $\mathbf{n}$ is an upper
bound on the \textbf{Expected} choice.  The \textbf{Logical} choice
will tend to over-estimate the maximum size of each prediction class,
but we might choose it over \textbf{Expected} if we have reason
to believe that it is difficult to estimate the baseline rate at
which the model is biased to predict certain labels by the label
distribution independent of the inputs.

\bmcomment{More detail on why Expected might be a better 
choice than Logical}

\section{Experiments}

\bmcomment{Add graphs showing convergence of SGD?}

We implemented the multiclass SVM and normalized cost learning SVM (SVMCLN) 
using stochastic (sub-)gradient descent (SGD) to approximate Objectives
\ref{svmObjective} and \ref{costObjective} (see \citep{yin2003stochastic} on
SGD).  Our SGD implementation
used a learning rate of $\frac{1}{\lambda_2 t}$ at time-step $t$ 
following the Pegasos algorithm in \citep{shalev2011pegasos}.
We ran several experiments to compare these models on standard
text classification corpora.  In each experiment, we compared 
the accuracies of the models on standard train/test split 
given by each respective
data corpus while also randomly selecting a 10\% subset of the 
training data to use as a dev set.  We used the dev set to perform
a grid search over 16 possible values ranging from $10^{-6}$ to 
$0.5\times 10^2$ for the $\lambda_2$ hyper-parameter in each model.
After the grid search, we fixed the best value for $\lambda_2$,
and retrained on the full training data,
evaluating the final performance of the model on the test set.  
 
Due to the stochasticity of the optimization algorithm, we 
expected some noisiness in its convergence.  In general, we 
ran SGD for 150 iterations over the random permutations
of the full data. After the 150 iterations, the accuracy and 
predictions of each model on the dev/test sets were relatively 
stable; the accuracy varied by at most $~0.001$ and 
fewer $5\%$ of the predictions changed 
during the last 10 iterations over the training 
data. 

\subsection{Datasets}

\bmcomment{Cite \url{http://www.aaai.org/Papers/AAAI/2006/AAAI06-121.pdf}
on relatively low difference in performance between log(tf) and tfidf}

\bmcomment{Cite \url{http://qwone.com/~jason/writing/loocv.pdf} for 
example use of log(tf)}

\bmcomment{Preprocessed using method from 
\url{http://web.ist.utl.pt/acardoso/docs/2007-phd-thesis.pdf}}

We compared the performance of the models on the 20 Newsgroups and
the Reuters-21578 (R52) data sets.  We chose these two data sets
because they both have a relatively large number of output labels,
some of which might not be distinctive enough for the SVM to plausibly
learn well given its features, in which case we might expect a gain in 
accuracy from SVMCLN.  As shown below, we found out that this 
expectation was correct for the 20 Newsgroups data, but incorrect for 
the Reuters data, for reasons that became apparent to us after we
reviewed the results.

For both of these data sets, the target labels $\mathcal{Y}$ are 
single document topics/categories, and the inputs $\mathcal{X}$ are
documents.  We preprocessed each text document from both corpora 
using the procedure given in [FILL IN] (convert to lower-case, 
remove symbols, etc), and then we computed $\mathbf{g}$ as normalized
$\log$ unigram term-frequency features.  In particular:

\begin{equation}
\mathbf{g}_{\mathbf{y'}}(\mathbf{x},\mathbf{y}) = \mathbbm{1}(\mathbf{y}=\mathbf{y'})\mathbf{f}(\mathbf{x})
\end{equation}

Where $\mathbf{f}(\mathbf{x})$ is a normalized vector whose elements 
are $\log(1+tf(\mathbf{x},v))$ when $v$ is a unigram in the corpus 
vocabulary and $tf(\mathbf{x},v)$ is the frequency of $v$ in document
 $\mathbf{x}$.

\subsubsection{Reuters 21578 (R52)}

The Reuters-21578 R52 corpus contains 9100 documents from the original
Reuters-21578 data, each of which is labeled with one of 52 possible topics.
\footnote{See 
\url{http://www.csmining.org/index.php/r52-and-r8-of-reuters-21578.html}.}  
The documents are divided along the Reuters-21578 'ModApte' split
into \~70\% training and \~30\% test.  The label distribution is extremely
non-uniform, with 43\% of the documents assigned to a single topic, and
71\% of the other topics each assigned fewer than 50 (0.5\%) documents.

\subsubsection{20 newsgroups}

The 'by date' version of the 20 Newsgroups data contains 
18846 documents sorted by date into 60\% for training and 40\% for testing.  
\footnote{See \url{http://qwone.com/~jason/20Newsgroups/}.} In comparison
to the Reuters data, the newsgroups distribution of 20 categories is
nearly uniform, with some topics highly related and some topics entirely
unrelated.  The relatedness of the topics is approximately captured by
their hierarchical naming scheme (e.g. there is \textit{rec.autos} and
\textit{rec.motorcycles} which are likely to be highly related to each
other, but mostly unrelated to \textit{sci.space}).  We expected that
this hierarchy would be encoded by the learned 'easiness' approximations,
since the 'easiness' with which the model discriminates two categories
likely decreases with their relatedness.

\subsection{Results}

\bmcomment{All 20news table numbers are wrong.  Need to fix them after experiments rerun.}

Table~\ref{accuraciesTable} shows the micro-averaged
accuracies on the Reuters and 20 newsgroups tasks for the 
SVM baseline model and versions of SVMCLN with
different choices of normalization constants 
$\mathbf{n}$ and incorrect prediction classes $\mathcal{S}$.  The last column 
in the table shows that none of the SVMCLN variations improve the
 ccuracy over the SVM on the Reuters task, but inspection of the 
confusion matrix for the SVM
baseline provides some intuition for this lack of improvement.  
The matrix shows
that $53\%$ of the SVM's mistakes were on examples with whose topics had 10 or
fewer test examples, all off which were predicted incorrectly.  Furthermore,
there is no non-diagonal element in the confusion matrix with more than 10 
mistakes.  These observations suggest that many of the SVM baseline's mistakes
come from infrequent labels rather than systematic conflations between certain
label pairs, and further that all of the incorrect prediction classes in the
cost learning model will be extremely small to begin with.  As a result, it's
unlikely that SVMCLN would be able to re-scale the costs to greatly shrink
any incorrect prediction classes.  In hindsight, this provides a good example
of a good example of how analysis of the errors made by the standard SVM can
determine whether cost learning will be beneficial.

In contrast, the fourth column of Table~\ref{accuraciesTable} shows that 
every SVMCLN variation improves the accuracy over the SVM on the 20 Newsgroups 
data.  Unsurprisingly, the \textbf{Logical} 
and \textbf{Expected} normalized versions
perform better than the non-normalized versions since they should give better
estimates of the 'easiness' measure and cost function.  Also, each 
$\mathcal{S}_{[\mathcal{Y}]^2}$ version performs slightly better than 
each $\mathcal{S}_{[\mathcal{Y}]^2}$ version, which makes sense given that
the ease of resolving mistakes in $S_{\mathbf{y},\mathbf{y}'}$ should be 
the same as the ease of resolving mistakes in $S_{\mathbf{y}',\mathbf{y}}$,
and so the ordering on $\mathbf{y}$ and $\mathbf{y}'$ gives the model
unnecessary extra parameters.  We also expected 
\textbf{Expected} to perform better 
than \textbf{Logical} since \textbf{Logical} 
over-estimates the maximum value of each incorrect prediction class, but
this expectation was not met with 
$\mathcal{S}_{[\mathcal{Y}]^2}$ prediction
classes, possibly because the \textbf{Expected} normalizers tend 
to under-estimate.

\begin{table}[t]
\caption{Micro-averaged Accuracies}
\label{accuraciesTable}
\begin{center}
\begin{tabular}{lllcccc}
\bf{Model}  & \bf{$\mathbf{n}$} & \bf{$\mathcal{S}$} & \bf{20news} & \bf{20news level 2}   & \bf{20news level 1}   & \bf{Reuters}
\\ \hline \\
SVM         & N/A               & N/A                & 0.7760      & 0.8008                & 0.8654                & 0.9213 \\
SVMCLN      & None              & $\mathcal{Y}^2$    & 0.8008      & 0.8259                & 0.8752                & 0.9213 \\ 
SVMCLN      & None              & $[\mathcal{Y}]^2$  & 0.8011      & 0.8257                & 0.8751                & 0.9194 \\
SVMCLN      & Logical           & $\mathcal{Y}^2$    & 0.8024      & 0.8271                & 0.8769                & 0.9210 \\ 
SVMCLN      & Logical           & $[\mathcal{Y}]^2$  & 0.8376      & 0.8631                & 0.9142                & 0.9159 \\
SVMCLN      & Expected          & $\mathcal{Y}^2$    & 0.8303      & 0.8557                & 0.9092                & 0.9159 \\ 
SVMCLN      & Expected          & $[\mathcal{Y}]^2$  & 0.8307      & 0.8558                & 0.9084                & 0.9171 \\
\end{tabular}
\end{center}
\end{table}

The hierarchical structure of the 20 Newsgroups topics encodes a notion
of distance between topics as distance within the hierarchy.  The fifth
and sixth columns of Table~\ref{accuraciesTable} show the accuracies 
computed when nearby topics in the hierarchy are are collapsed into
single topics--the fifth column shows the accuracies computed when all
topics that are the same to two levels deep in the hierarchy are collapsed
into a single topic, and the sixth column shows the accuracies computed
when all topics that are the same at the first level of the hierarchy are
collapsed into a single topic.  

\bmcomment{I think there are ways to make the following result clearer
if we compute other numbers... but I don't know if we have time for that.
This may be a bit confusing I think though}

The cost learning should learn a notion of distance between topics 
approximated by the cost function, and we expect this notion of distance
to approximate the notion of distance encoded by the hierarchy.  The 
approximated notion of distance encoded in the cost function should 
cause SVMCLN to improve at distinguishing topics that it estimates to
be far apart from each other.  So if SVMCLN's learned distances 
approximate the distances through the hierarchy, then we should expect the SVMCLN's collapsed 
accuracy improvements to be nearly as great as the uncollapsed accuracy
improvements.  This is shown by the [FILL] accuracy improvements in the
fourth, fifth, and sixth columns of Table~\ref{accuraciesTable} by
the \textbf{Logical} $\mathcal{S}_{[\mathcal{Y}]^2}$ version of SVMCLN.

\bmcomment{Fix the above paragraph when get new numbers}

\bmcomment{Cite hierarchical clustering.  Be specific about which
type of hierarchical clustering is used.}

\bmcomment{Add hierarchy figure.}

We also directly evaluated the extent to which the cost function approximates
the newsgroup hierarchy by constructing a hierarchy from the 
'easiness' approximations and comparing it to the newsgroup hierarchy.
In order to construct the hierarchy, we ran hierarchical clustering on
the newsgroups where each approximated 'easiness' value acts as a
distance measure.  The approximate hierarchy for the \textbf{Logical}
$\mathcal{S}_{[\mathcal{Y}]^2}$ SVMCLN is shown in figure [FILL], and the true
Newsgroup hierarchy is shown in figure [FILL].  

\bmcomment{Add measurements of similarity between hierarchies}

% Learned hierarchy

\begin{tikzpicture}
\tikzset{font=\footnotesize}
\tikzset{anchor=base}
\tikzset{sibling distance=50pt}
\tikzset{frontier/.style={distance from root=200}}
\tikzset{grow=right}
%\tikzset{every tree node/.style={anchor=base west}}
\tikzset{edge from parent/.style={draw, edge from parent fork right}}
\Tree [ [ the cat ] [ the dog ] ]
\end{tikzpicture}
%
%\begin{tikzpicture}[sloped]
%\node (a) at (-6,0) {a};
%\node (b) at (-3,0) {b};
%\node (c) at (-0.5,0) {c};
%\node (d) at (0.5,0) {d};
%\node (e) at (2,0) {e};
%\node (ab) at (-4.5,3) {};
%\node (cd) at (0,1) {};
%\node (cde) at (1,2) {};
%\node (all) at (-1.5,5) {};
% 
%\draw  (a) |- (ab.center);
%\draw  (b) |- (ab.center);
%\draw  (c) |- (cd.center);
%\draw  (d) |- (cd.center);
%\draw  (e) |- (cde.center);
%\draw  (cd.center) |- (cde.center);
%\draw  (ab.center) |- (all.center);
%\draw  (cde.center) |- (all.center);
% 
%\draw[->,-triangle 60] (-7,0) -- node[above]{distance} (-7,6);
%\end{tikzpicture}


\section{Discussion}

\bmcomment{Maybe add summary of conclusions here or at the 
very end...?}

\subsection{Related literature}

\bmcomment{Here are some things to possibly write about:}

Self-paced learning \citep{kumar2010self}.

Curriculum Learning \citep{bengio2009curriculum}.

Confidence weighted learning \citep{dredze2008confidence}.

Ed Hovy inter-annotator agreement cost \citep{plank2014learning}

Hidden variable learning by state splitting mentioned in 
\url{http://www.cs.cmu.edu/~nasmith/papers/career-proposal-2010.pdf} 
\citep{petrov2011coarse}.

Finite state output encodings mentioned in 
\url{http://www.cs.cmu.edu/~nasmith/papers/career-proposal-2010.pdf}
\citep{loper2008encoding}

\subsection{Future work}

\bmcomment{Is there anything else that I forgot?}

We made several choices within the present work for which
many alternatives might be interesting to pursue in
future research.  We proposed two choices for 
sets $\mathcal{S}$ of incorrect prediction classes which 
bucket predictions based on label pairs, but many others
are possible. For example, we might consider construct
prediction classes for varying frequencies of the labels
in the training data, motivated by the idea that incorrect
predictions involving frequently occurring labels are easier
to resolve than incorrect predictions involving infrequently
occurring labels.  

Another idea is to change the cost function
to incorporate the input $\mathbf{x}_i$ as 
$\Delta(\mathbf{x}_i,\mathbf{y}_i,\mathbf{y})$, This would
allow the incorrect prediction classes and their 'easiness' 
estimates to depend on features of the input--which would
possibly be beneficial to the model for learning and to 
the model engineer for error analysis.

We chose to explore the straightforward \textbf{Logical} 
and \textbf{Expected} values for $\mathbf{n}$, but we might 
also consider using a more sophisticated model to estimate the 
irrelevant biases in the sizes of the incorrect prediction 
classes.

Our cost learning model was build for multiclass classification
tasks, but it should be easy to extend to structured prediction
tasks.

\bmcomment{Noah, would it be good to talk about some of the
learning output spaces ideas from your career proposal in 
relation to the structured possibility here?}

In the more general theme of cost function learning, it's possible
to choose alternative measures of 'easiness' that do not directly
depend on the sizes of the prediction classes and could require
changing the model to an entirely different form.  For example,
we might make easiness depend on some measure of linear separability
between each label pair, or generalizability of predictions involving
certain labels. The generalizability of certain predictions might 
be characterized by some measure like 'stability' from learning 
theory \citep{mukherjee2006learning}.

\bmcomment{The comment about stability might be entirely off base
because I don't know anything about it, but it was something
I wanted to look into.}

There are also several unanswered theoretical questions related 
to this work.  We have used scare-quotes around 'easiness' throughout
this paper because we aren't using the term in an unambiguous way,
and we don't have a single precise concept in mind.  There are
several concepts of 'easiness' related to, e.g., the reliability 
of the annotated labels, the distinctiveness of the labels given the 
choice of features, the distinctiveness of the labels given
the choice of model, the learnability of each label given its
frequency, or the similarity between the distribution of the
training data and the true distribution. There are several ways we 
might model these notions of 'easiness' by the choice of 
model and the categorization of incorrect predictions into classes.
It would be interesting to describe a taxonomy of different kinds
of 'easiness' and ways in which they might be learned.  Furthermore,
it would be useful to find a systematic way to predict whether 
learning a given notion of 'easiness' on a data set will allow a model 
to improve its accuracy.  Analyzing the confusion matrix for the
Reuter's data helped us make some guesses about why the notion of 'easiness' 
captured by our model did not help improve its performance, but
there might be ways of making our intuitions about this more systematic.

\bmcomment{Possibly move parts of the previous paragraph to 
the background/introduction sections. Also might want to reword it to
make clearer, or just talk about it in a different way.  At least
make it shorter...}

%\subsubsection*{Acknowledgments}

%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments go at the end of the paper. Do not include 
%acknowledgments in the anonymized submission, only in the 
%final paper. 

\bmcomment{The 'references' heading given by the 
bibliography command is the wrong size font.  Needs to
be the size of a 'third level heading'.  How to change this?}

\bibliography{refs}

\end{document}
